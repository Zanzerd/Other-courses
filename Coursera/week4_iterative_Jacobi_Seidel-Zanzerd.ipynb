{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple iteration for systems of linear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate a random diagonally dominant matrix, for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rndm = np.random.RandomState(1234)\n",
    "\n",
    "n = 10\n",
    "A = rndm.uniform(size=(n, n)) + np.diagflat([15]*n)\n",
    "b = rndm.uniform(size=n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.  Jacobi iteration\n",
    "\n",
    "Given\n",
    "\n",
    "$$\n",
    "A x = b\n",
    "$$\n",
    "\n",
    "separate the diagonal part $D$,\n",
    "\n",
    "$$ A = D + (A - D) $$\n",
    "\n",
    "and write\n",
    "\n",
    "$$\n",
    "x = D^{-1} (D - A) x + D^{-1} b\\;.\n",
    "$$\n",
    "\n",
    "Then iterate\n",
    "\n",
    "$$\n",
    "x_{n + 1} = B x_{n} + c\\;,\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "B = D^{-1} (D - A) \\qquad \\text{and} \\qquad c = D^{-1} b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the matrix and the r.h.s. for the Jacobi iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_1d = np.diag(A)\n",
    "\n",
    "B = -A.copy()\n",
    "np.fill_diagonal(B, 0)\n",
    "\n",
    "D = np.diag(diag_1d)\n",
    "invD = np.diag(1./diag_1d)\n",
    "BB = invD @ B \n",
    "c = invD @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "assert_allclose(-B + D, A)\n",
    "\n",
    "\n",
    "# xx is a \"ground truth\" solution, compute it using a direct method\n",
    "xx = np.linalg.solve(A, b)\n",
    "\n",
    "np.testing.assert_allclose(A@xx, b)\n",
    "np.testing.assert_allclose(D@xx, B@xx + b)\n",
    "np.testing.assert_allclose(xx, BB@xx + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that $\\| B\\| \\leqslant 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36436161983015336"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(BB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the Jacobi iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50\n",
    "\n",
    "x0 = np.ones(n)\n",
    "x = x0\n",
    "for _ in range(n_iter):\n",
    "    x = BB @ x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  2.22044605e-16,  0.00000000e+00, -1.11022302e-16,\n",
       "        0.00000000e+00,  0.00000000e+00, -2.08166817e-17,  0.00000000e+00,\n",
       "        0.00000000e+00,  2.22044605e-16])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the result:\n",
    "\n",
    "A @ x - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task I.1\n",
    "\n",
    "Collect the proof-of-concept above into a single function implementing the Jacobi iteration. This function should receive the r.h.s. matrix $A$, the l.h.s. vector `b`, and the number of iterations to perform.\n",
    "\n",
    "\n",
    "The matrix $A$ in the illustration above is strongly diagonally dominant, by construction. \n",
    "What happens if the diagonal matrix elements of $A$ are made smaller? Check the convergence of the Jacobi iteration, and check the value of the norm of $B$.\n",
    "\n",
    "(20% of the total grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacobi_iter(A, b, max_iter):\n",
    "    diag_1 = np.diag(A)\n",
    "    B = -A.copy()\n",
    "    np.fill_diagonal(B, 0)\n",
    "    D = np.diag(diag_1)\n",
    "    invD = np.diag(1./diag_1)\n",
    "    BB = invD @ B \n",
    "    c = invD @ b\n",
    "    \n",
    "    x = np.ones(A.shape[0])\n",
    "    for _ in range(max_iter):\n",
    "        x = BB @ x + c\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  2.22044605e-16,  0.00000000e+00, -1.11022302e-16,\n",
       "        0.00000000e+00,  0.00000000e+00, -2.08166817e-17,  0.00000000e+00,\n",
       "        0.00000000e+00,  2.22044605e-16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = Jacobi_iter(A, b, 50)\n",
    "\n",
    "A @ x2 - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.287232396603068"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[np.diag_indices_from(A)] /= 20 #make diagonal elements of A smaller\n",
    "\n",
    "diag_2 = np.diag(A)\n",
    "B2 = -A.copy()\n",
    "np.fill_diagonal(B2, 0)\n",
    "D2 = np.diag(diag_2)\n",
    "invD2 = np.diag(1./diag_2)\n",
    "BB2 = invD2 @ B2\n",
    "\n",
    "np.linalg.norm(BB2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition $$ \\|B\\| < 1 $$ is equivalent to A being row diagonally dominant, and is a sufficient condition for convergence. So, Jacobi method won't converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.82809888e+78, 3.34232163e+78, 3.16535310e+78, 3.94548471e+78,\n",
       "       2.07976084e+78, 3.34791701e+78, 2.11693907e+78, 2.58900771e+78,\n",
       "       3.49906469e+78, 3.04248192e+78])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = Jacobi_iter(A, b, 100)\n",
    "\n",
    "A @ x3 - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These NaNs appear due to overflow (machine infinity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Seidel's iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task II.1\n",
    "\n",
    "Implement the Seidel's iteration. \n",
    "\n",
    "Test it on a random matrix. Study the convergence of iterations, relate to the norm of the iteration matrix.\n",
    "\n",
    "(30% of the total grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndm = np.random.RandomState(1234)\n",
    "\n",
    "n = 10\n",
    "#A = rndm.uniform(size=(n, n))\n",
    "A = rndm.randint(0, 100000, size = (n,n))\n",
    "#b = rndm.uniform(size=n)\n",
    "b = rndm.randint(0, 100000, size = n)\n",
    "A = np.tril(A)  #creating a triangilar matrix for Seidel's method to converge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Seidel_iter(A, b, max_iter):\n",
    "    B = A.copy()\n",
    "    diag_1 = np.diag(B)\n",
    "    D = np.diag(diag_1)\n",
    "    L = np.tril(B, -1) #lower triangular matrix with zeros on the main diagonal\n",
    "    U = np.triu(B, 1)\n",
    "    DpL_inv = np.linalg.inv(D + L)\n",
    "    BB = DpL_inv @ U\n",
    "    c = DpL_inv @ b\n",
    "    x = np.ones(B.shape[0])\n",
    "    for _ in range(max_iter):\n",
    "        x = -BB @ x + c\n",
    "    return x, BB #return BB only to check conditions of convergence later on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.18278728e-11,  1.45519152e-11, -5.82076609e-11, -5.82076609e-11,\n",
       "       -1.74622983e-10,  0.00000000e+00,  3.49245965e-10, -4.65661287e-10,\n",
       "       -1.16415322e-10, -2.32830644e-10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, BB = Seidel_iter(A, b, 100)\n",
    "\n",
    "A @ x - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(BB, ord = np.inf))\n",
    "print(np.linalg.norm(BB, ord = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both inf-norm and 1-norm are zeroes because our BB matrix values are so small that they underflow to zero even if matrix A has values like 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Minimum residual scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task III.1\n",
    "\n",
    "Implement the $\\textit{minimum residual}$ scheme: an explicit non-stationary method, where at each step you select the iteration parameter $\\tau_n$ to minimize the residual $\\mathbf{r}_{n+1}$ given $\\mathbf{r}_n$. Test it on a random matrix, study the convergence to the solution, in terms of the norm of the residual and the deviation from the ground truth solution (which you can obtain using a direct method). Study how the iteration parameter $\\tau_n$ changes as iterations progress.\n",
    "\n",
    "(50% of the grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_res(A, b, max_iter):\n",
    "    A1 = A.copy()\n",
    "    x = np.ones(A1.shape[0])\n",
    "    for i in range(max_iter):\n",
    "        res_n = A1 @ x - b\n",
    "        norm = np.linalg.norm(A1 @ res_n)\n",
    "        if norm == 0:\n",
    "            break\n",
    "        tau_next = (res_n @ A1 @ res_n) / (norm ** 2) \n",
    "        #print(tau_next)\n",
    "        x = x - (tau_next * res_n) \n",
    "    return x, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:99, root:[ 1.79661141e-02  2.55766053e-01  2.85574376e-02  1.57003318e-01\n",
      "  6.10438645e-02  8.66272382e-02 -3.45867931e-05  1.81619055e-01\n",
      "  9.10831437e-02  6.63299471e-02]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.05966419, -0.24831948,  0.42149358,  0.40607066,  0.13451906,\n",
       "       -0.06066065, -0.40232821, -0.05220382, -0.40338393,  0.22558069])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "rndm = np.random.RandomState(123)\n",
    "A = rndm.uniform(size=(n, n))\n",
    "b = rndm.uniform(size = n) \n",
    "\n",
    "\n",
    "x, it = min_res(A, b, 100)\n",
    "\n",
    "print(\"Iterations:\" + str(it) + \", root:\" + str(x))\n",
    "A @ x - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My method behaves badly when i simply fill matrix A with random integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02236172,  0.01696515,  0.07397027, -0.16209987, -0.07984139,\n",
       "       -0.45673867, -0.24160662,  0.10442583,  0.03052831,  0.60363515])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.tril(A)\n",
    "\n",
    "x, it = min_res(A, b, 1000)\n",
    "A @ x - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks better when we use Seidel method's condition - a triangular matrix, but it's still bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:23, root:[0.00245413 0.04541407 0.00331114 0.00047118 0.0571877  0.03483625\n",
      " 0.007839   0.03672017 0.01350848 0.00216622]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.11022302e-16,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rndm.uniform(size=(n, n)) + np.diagflat([15]*n)\n",
    "b = rndm.uniform(size=n)\n",
    "\n",
    "print(\"Iterations:\" + str(it) + \", root:\" + str(x))\n",
    "x, it = min_res(A, b, 100)\n",
    "A @ x - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works quite well with diagonally dominant matrix A. The method rarely uses all iterations since the norm of the residual quickly overflows to zero (or underflows to zero, idk how to say it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example the coefficient tau rapidly overflows to zero. In the second example it takes on a limited set of values and changes its sign so actually it doesn't do anything and the method treads water. In the third example it changes slightly at each step, starting at 0.048949 and stopping at 0.062082 on the 20-th iteration (after that it remains constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our solution: [ 0.050477   -0.00539249  0.03884936  0.03354195  0.02830406  0.05061043\n",
      "  0.03307114  0.04814944  0.04373544  0.04540488]\n",
      "Ground truth solution:[ 0.050477   -0.00539249  0.03884936  0.03354195  0.02830406  0.05061043\n",
      "  0.03307114  0.04814944  0.04373544  0.04540488]\n",
      " \n",
      "The difference:  2.7755575615628914e-17\n"
     ]
    }
   ],
   "source": [
    "xx = np.linalg.solve(A, b) \n",
    "print(\"Our solution: \" + str(x) + \"\\nGround truth solution:\" + str(xx))\n",
    "print(\" \\nThe difference: \", str(abs(np.linalg.norm(x) - np.linalg.norm(xx))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
